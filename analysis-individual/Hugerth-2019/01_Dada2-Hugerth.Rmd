---
title: "Dada2_Hugherth2020"
params:
  date: "!r Sys.Date()"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r, include=FALSE}
knitr::opts_chunk$set(fig.width = 3.5, fig.height = 3, warning=FALSE, message=FALSE,
                      root.dir = "~/Projects/IBS_Meta-analysis_16S/")
```


# Import library
```{r library-import}
library(dada2)
packageVersion("dada2") # check dada2 version

library(Biostrings)
library(ShortRead)
library(seqTools) # per base sequence content
library(phyloseq)
library(ggplot2)
library(data.table)
library(plyr)
library(dplyr)
library(qckitfastq) # per base sequence content
library(stringr)
```


#####################
##  QUALITY CHECK  ##
#####################

First, we import the fastq files containing the raw reads. The samples were downloaded from the ENA database with the accession number PRJEB31817.

```{r quality-check, echo=TRUE, results="hide"}
# Save the path to the directory containing the fastq zipped files
path <- "~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019"
list.files(path) # check we are in the right directory

# fastq filenames have format: SAMPLENAME.fastq.gz
# Saves the whole directory path to each file name
fnFs <- sort(list.files(file.path(path, "original"), pattern="_1.fastq.gz", full.names = TRUE)) # forward
fnRs <- sort(list.files(file.path(path, "original"), pattern="_2.fastq.gz", full.names = TRUE)) # reverse
show(fnFs[1:5])
show(fnRs[1:5])
#length(fnFs) # should be 747

# Extract sample names, assuming filenames have format: SAMPLENAME.fastq.gz
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
show(sample.names[1:5]) # saves only the file name (without the path)

# Look at quality of all files
for (i in 1:3){ # 1:length(fnFs)
  show(plotQualityProfile(fnFs[i]))
  show(plotQualityProfile(fnRs[i]))
}

# Look at nb of reads per sample
#raw_stats <- data.frame('sample' = sample.names,
#                        'reads' = fastqq(fnFs)@nReads)
#min(raw_stats$reads)
#max(raw_stats$reads)
#mean(raw_stats$reads)
```

We will have a quick peak at the per base sequence content of the reads in some samples, to make sure there is no anomaly (i.e. all reads having the same sequence).

```{r per-base-seq-content, fig.width = 6, fig.height = 3}
# Look at per base sequence content (forward read)
fseqF <- seqTools::fastqq(fnFs[1])
rcF <- read_content(fseqF)
plot_read_content(rcF) + labs(title = "Per base sequence content - Forward read")

# Look at per base sequence content (reverse read)
fseqR <- seqTools::fastqq(fnRs[1])
rcR <- read_content(fseqR)
plot_read_content(rcR) + labs(title = "Per base sequence content - Reverse read")
```


```{r pdf-import, echo = FALSE}
#__________________________________________________________________________
#____________ THIS IS ONLY FOR (quicker) PDF/HTML OUTPUT __________________
#__________________________________________________________________________

# seq depth
raw_stats <- readRDS("~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/01_Dada2-Hugerth/raw_stats.rds")
# out process
FWD.out1 <- readRDS("~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/01_Dada2-Hugerth/FWD_out1.rds")
REV.out1 <- readRDS("~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/01_Dada2-Hugerth/REV_out1.rds")
out2 <- readRDS("~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/01_Dada2-Hugerth/out2.rds")
track <- readRDS("~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/01_Dada2-Hugerth/track.rds")
# primer removal
FWD.filt1_samples <- sort(list.files(file.path(path, "filtered1"), pattern="_1_filt.fastq.gz", full.names = TRUE))
REV.filt1_samples <- sort(list.files(file.path(path, "filtered1"), pattern="_2_filt.fastq.gz", full.names = TRUE))
# quality profiles
FWD.filt2_samples <- sort(list.files(file.path(path, "filtered2"), pattern="_1_filt.fastq.gz", full.names = TRUE))
REV.filt2_samples <- sort(list.files(file.path(path, "filtered2"), pattern="_2_filt.fastq.gz", full.names = TRUE))
# error rates
errF <- readRDS("~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/01_Dada2-Hugerth/errF.rds")
errR <- readRDS("~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/01_Dada2-Hugerth/errR.rds")
# infered ASVs
dadaFs <- readRDS("~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/01_Dada2-Hugerth/infered_seq_F.rds")
dadaRs <- readRDS("~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/01_Dada2-Hugerth/infered_seq_R.rds")
# mergers
mergers <- readRDS("~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/01_Dada2-Hugerth/mergers.rds")
# taxa
taxa <- readRDS("~/Projects/IBS_Meta-analysis_16S/data/OutputTaxa/taxa_hugerth.rds")
# phyloseq object
physeq <- readRDS("~/Projects/IBS_Meta-analysis_16S/phyloseq-objects/physeq_hugerth.rds")
```


########################
##  LOOK FOR PRIMERS  ##
########################

Now, we will look whether the reads still contain the primers. Primer sequences could be found in the SRARunTable (metadata table from SRA).

```{r primer-check, eval = TRUE, message=FALSE}
# V3-V4
FWD <- "CCTACGGGNGGCWGCAG"  # 341F primer sequence
REV <- "GACTACHVGGGTATCTAATCC" # 805R primer sequence

# Function that, from the primer sequence, will return all combinations possible (complement, reverse complement, ...)
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
                 RevComp = reverseComplement(dna)) # get all orientations
    return(sapply(orients, toString))  # Convert back to character vector
}

# Get all combinations of the primer sequences
FWD.orients <- allOrients(FWD) # 341F
REV.orients <- allOrients(REV) # 805R
FWD.orients # sanity check
REV.orients

# Function that counts number of reads in which a sequence is found
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE, max.mismatch = 2)
    return(sum(nhits > 0))
}

# Look in all samples how many times the 341F and 805R primers are found in the reads of each sample
for (i in 1:5){
  cat("SAMPLE", sample.names[i], "with total number of", raw_stats[i,'reads'], "reads\n\n")
  x <- rbind(ForwardRead.FWDPrimer = sapply(FWD.orients, primerHits, fn = fnFs[[i]]),
             ForwardRead.REVPrimer = sapply(REV.orients, primerHits, fn = fnFs[[i]]),
             ReverseRead.FWDPrimer = sapply(FWD.orients, primerHits, fn = fnRs[[i]]), 
             ReverseRead.REVPrimer = sapply(REV.orients, primerHits, fn = fnRs[[i]]))
  print(x)
  cat("\n____________________________________________\n\n")
}
```



#####################
## FILTER AND TRIM ##
#####################

### 1. Primer removal

The reads indeed contain the forward primer in the forward reads, and the reverse primer in the reverse reads. We will keep only reads containing the primer, and then remove the primer!

```{r primer-filter, eval = FALSE, echo = TRUE}
# KEEP READS WITH PRIMER AND REMOVE PRIMER+BARCODE
# Place filtered files in a filtered1/ subdirectory
FWD.filt1_samples <- file.path(path, "filtered1", paste0(sample.names, "_1_filt.fastq.gz")) # FWD reads
REV.filt1_samples <- file.path(path, "filtered1", paste0(sample.names, "_2_filt.fastq.gz")) # REV reads
# Assign names for the filtered fastq.gz files
names(FWD.filt1_samples) <- sample.names
names(REV.filt1_samples) <- sample.names

# Keep only reads with primers & remove primers
FWD.out1 <- removePrimers(fn = fnFs, fout = FWD.filt1_samples,
                          primer.fwd = FWD,
                          trim.fwd = TRUE,
                          orient = FALSE, # re-orient reads if needed
                          compress = TRUE, verbose = TRUE)
REV.out1 <- removePrimers(fn = fnRs, fout = REV.filt1_samples,
                          primer.fwd = REV,
                          trim.fwd = TRUE,
                          orient = FALSE, # re-orient reads if needed
                          compress = TRUE, verbose = TRUE)

```

```{r primer-filter-check, eval = FALSE, include = FALSE, echo = FALSE}
# Primer removal
FWD.out1[1:3,]
REV.out1[1:3,]

# Quality profile after primer removal
for (i in 1:3){
  show(plotQualityProfile(FWD.filt1_samples[i]))
  show(plotQualityProfile(REV.filt1_samples[i]))
}
```

```{r primer-filter-check2, eval=FALSE, include=FALSE, echo=FALSE}
# Check primers were removed
for (i in 1:5){
  cat("SAMPLE ", sample.names[i], "\n")
  # Get a table to know how many times the 341F and 805R primers are found (in how many reads)
  x <- rbind(ForwardRead.FWDPrimer = sapply(FWD.orients, primerHits, fn = FWD.filt1_samples[[i]]),
             ForwardRead.REVPrimer = sapply(REV.orients, primerHits, fn = FWD.filt1_samples[[i]]),
             ReverseRead.FWDPrimer = sapply(FWD.orients, primerHits, fn = REV.filt1_samples[[i]]), 
             ReverseRead.REVPrimer = sapply(REV.orients, primerHits, fn = REV.filt1_samples[[i]]))
  print(x)
  cat("\nTotal number of reads: ", fastqq(filt1_fnFs[i])@nReads)
  cat("\n____________________________________________\n\n")
}
```



### 2. Quality filtering

Then, we perform a quality filtering of the reads. 

```{r filter-trim, eval = FALSE, echo = TRUE}
# Place filtered files in a filtered/ subdirectory
FWD.filt2_samples <- file.path(path, "filtered2", paste0(sample.names, "_1_filt.fastq.gz")) # FWD reads
REV.filt2_samples <- file.path(path, "filtered2", paste0(sample.names, "_2_filt.fastq.gz")) # REV reads
# Assign names for the filtered fastq.gz files
names(FWD.filt2_samples) <- sample.names
names(REV.filt2_samples) <- sample.names

# Create matrix that will count the reads input & reads output (after quality filter)
out2 <- NULL
out2 <- matrix(0L, nrow=length(747), ncol=2)
colnames(out2) <- c("reads.in", "reads.out")


# Quality filter
for (i in 1:747){
  
  tryCatch({
    out <- NULL # initialize
    out <- filterAndTrim(fwd = filt1_fnFs[i], filt = FWD.filt2_samples[i],
                        rev = filt1_fnRs[i], filt.rev = REV.filt2_samples[i],
                        maxEE=4, # reads with more than 4 expected errors (sum(10e(-Q/10))) are discarded
                        truncQ=10, # Truncate reads at the first instance of a quality score less than or equal to truncQ.
                        minLen = 200, # Discard reads shorter than 200 bp. This is done after trimming and truncation.
                        matchIDs = TRUE, # match forward & reverse reads (discard reads that don't have their pair)
                        multithread = TRUE,
                        compress=TRUE, # Output files in gzip format
                        verbose=TRUE) # give a message saying the percentage of filtered sequences
    out2 <- rbind(out2, out)
    #print(out2)
    #print("-------")
    #cat(rownames(out))
  }, error=function(e){cat("ERROR :", conditionMessage(e), "\n")})
  
}


# Remove first row that only has 0
out2 <- out2[-1,]

# 3 files had an error, which ones?
setdiff(sample.names, sub("_.*", "", rownames(out2)))
# Output: ERR3586005, ERR3586042, ERR3586312
# When looking at the header of the gzip files, they have "@1" written, and not
# "@HWI-M03284:58:000000000-B4YML:1:1101:20603:1834" that allow to match forward & reverse reads
```


Let's look at the output filtered fastq files as sanity check.

```{r filter-check}
out2[1:4,] # show how many reads were filtered in each file

# Look at quality profile of all filtered files
for (i in 1:3){
  show(plotQualityProfile(FWD.filt2_samples[i]))
  show(plotQualityProfile(REV.filt2_samples[i]))
}
```



#######################
## LEARN ERROR RATES ##
#######################

Now we will build the parametric error model, to be able to infer amplicon sequence variants (ASVs) later on.

```{r error-rate-estimate, eval = FALSE}
errF <- learnErrors(FWD.filt2_samples, multithread=TRUE, randomize = TRUE, verbose = 1)
errR <- learnErrors(REV.filt2_samples, multithread=TRUE, randomize = TRUE, verbose = 1)
```

The error rates for each possible transition (A→C, A→G, …) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.

```{r error-rate-check, include=FALSE, echo=FALSE, eval=FALSE}
# sanity check on errF
dada2:::checkConvergence(errF)
```

```{r plot-errors, fig.height=5, fig.width=5}
plotErrors(errF, nominalQ = TRUE) # Forward reads
plotErrors(errR, nominalQ = TRUE) # Reverse reads
```


##############################
## CONSTRUCT SEQUENCE TABLE ##
##############################

### 1. Infer sample composition

The _dada()_ algorithm infers sequence variants based on estimated errors (previous step). Firstly, we de-replicate the reads in each sample, to reduce the computation time.
De-replication is a common step in almost all modern ASV inference (or OTU picking) pipelines, but a unique feature of derepFastq is that it maintains a summary of the quality information for each dereplicated sequence in $quals.

```{r infer-sample-composition, eval = FALSE}
# Dereplicate the reads in the sample
derepF <- derepFastq(FWD.filt2_samples) # forward
derepR <- derepFastq(REV.filt2_samples) # reverse

# Infer sequence variants
dadaFs <- dada(derepF, err=errF, multithread=TRUE) # forward
dadaRs <- dada(derepR, err=errR, multithread=TRUE) # reverse

```

```{r}
# Inspect the infered sequence variants from sample 1:2
for (i in 1:2){
  print(dadaFs[[i]])
  print(dadaRs[[i]])
  print("________________")
}
```

### 2. Merge paired-end reads

```{r merge-paired-end, eval = FALSE}
mergers <- mergePairs(dadaFs, derepF, dadaRs, derepR, verbose=TRUE)
```

```{r}
head(mergers[[1]])
```


### 3. Construct ASV table

We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.

```{r build-seq-table, fig.height=3, fig.width=4}
# Make sequence table from the infered sequence variants
seqtable <- makeSequenceTable(mergers)

# We should have 744 samples (744 rows)
dim(seqtable)

# Inspect distribution of sequence lengths
hist(nchar(getSequences(seqtable)), breaks = 100, xlab = "Read length", ylab = "Number of reads", main="")

# Sequences should be between 341F - 805R, so around 427bp (removing the primers lengths).
# Considering that we kept only reads containing the primers, and merged the reads, we shouldn't have sequences below ~400bp

# Remove any sequence variant below outside 400-450bp
seqtable <- seqtable[,nchar(colnames(seqtable)) %in% 400:450]
dim(seqtable)
hist(nchar(getSequences(seqtable)), breaks = 100, xlab = "Read length", ylab = "Number of reads", main="")
```


### 4. Remove chimeras

The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

```{r remove-chimeras}
seqtable.nochim <- removeBimeraDenovo(seqtable, method="consensus", multithread=TRUE, verbose=TRUE)

# Check how many sequence variants we have after removing chimeras
dim(seqtable.nochim)

# Check how many reads we have after removing chimeras (we should keep the vast majority of the reads, like > 80%)
sum(seqtable.nochim)/sum(seqtable)
```



##########################################
## LOOK AT READS COUNT THROUGH PIPELINE ##
##########################################

Sanity check before assigning taxonomy.

```{r reads-filtered-sanity-check, echo=TRUE, eval=FALSE}
# Function that counts nb of reads
getN <- function(x) sum(getUniques(x))

# Table that will count number of reads for each process of interest (input reads, filtered reads, denoised reads, non chimera reads)
input <- raw_stats$reads[raw_stats$sample %in% rownames(seqtable.nochim)]
track <- cbind(input,
               out2[,2],
               sapply(dadaFs, getN),
               sapply(dadaRs, getN),
               sapply(mergers, getN),
               rowSums(seqtable.nochim),
               lapply(rowSums(seqtable.nochim)*100/input, as.integer))

# Assign column and row names
colnames(track) <- c("input",
                     "quality-filt", "denoisedF", "denoisedR",
                     'merged', 'nonchim', "%kept_input->output")
rownames(track) <- rownames(seqtable.nochim)
```

```{r}
# Show final table: for each row/sample, we have shown the initial number of reads, filtered reads, denoised reads, and non chimera reads
track[1:100,]
```




#####################
## ASSIGN TAXONOMY ##
#####################

Extensions: The dada2 package also implements a method to make species level assignments based on exact matching between ASVs and sequenced reference strains. Recent analysis suggests that exact matching (or 100% identity) is the only appropriate way to assign species to 16S gene fragments. Currently, species-assignment training fastas are available for the Silva and RDP 16S databases. To follow the optional species addition step, download the silva_species_assignment_v132.fa.gz file, and place it in the directory with the fastq files.

```{r assign-taxonomy, eval = FALSE}
# Assign taxonomy (with silva v138)
taxa <- assignTaxonomy(seqtable.nochim, "~/Projects/IBS_Meta-analysis_16S/data/silva_nr_v138_train_set.fa.gz",
                       tryRC = TRUE, # try reverse complement of the sequences
                       multithread=TRUE, verbose = TRUE)

# Add species assignment
taxa <- addSpecies(taxa, "~/Projects/IBS_Meta-analysis_16S/data/silva_species_assignment_v138.fa.gz")
```

```{r}
# Check how the taxonomy table looks like
taxa.print <- taxa
rownames(taxa.print) <- NULL # Removing sequence rownames for display only
head(taxa.print)

table(taxa.print[,1]) # Show the different kingdoms (should be only bacteria)
table(taxa.print[,2]) # Show the different Phyla
table(is.na(taxa.print[,2])) # is there any NA phyla?
```

########################
## LAST PREPROCESSING ##
########################

We will remove any sample with less than 500 reads from further analysis, and also any ASVs with unassigned phyla.

### 1. Create phyloseq object
The preprocessing will be easier to do with ASV, taxonomic and metadata tables combined in a phyloseq object.

```{r phyloseq-preprocess, echo=TRUE, eval=FALSE}
#_________________________
# Remove .fastq.gz extension in seqtable.nochim
rownames(seqtable.nochim) <- gsub("_1_filt.fastq.gz", "", rownames(seqtable.nochim))

#________________________
# Import metadata
metadata_table <- read.csv("~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/modif_metadata(R).csv",
                           sep=" ")
metadata_table$collection_date <- NULL
colnames(metadata_table)[12] <- "collection_date"
metadata_table[] <- lapply(metadata_table, function(x) if(is.factor(x)) as.character(x) else x)
metadata_table$host_disease[metadata_table$host_disease == "none"] <- "Healthy"
rownames(metadata_table) <- metadata_table$Run
head(metadata_table)

metadata_table$author <- "Hugerth"
metadata_table$sequencing_tech <- "Illumina"
metadata_table$variable_region <- "V3-V4"

#_________________________
# Create phyloseq object
physeq <- phyloseq(otu_table(seqtable.nochim, taxa_are_rows=FALSE),
                   sample_data(metadata_table[metadata_table$Run %in% rownames(seqtable.nochim),]),
                   tax_table(taxa))

# Remove samples with less than 500 reads
physeq <- prune_samples(sample_sums(physeq)>=500, physeq) # 85 samples discarded
# Remove taxa that are eukaryota, or have unassigned Phyla
physeq <- subset_taxa(physeq, Kingdom != "Eukaryota")
physeq <- subset_taxa(physeq, !is.na(Phylum))
```


```{r phyloseq-preprocess-2, echo=FALSE, eval=FALSE}
# Phyloseq object (including phylogenetic tree) was built without metadata
physeq <- readRDS("~/Projects/IBS_Meta-analysis_16S/data/OutputPhangorn/physeq_hugerth.rds")
physeq #659 samples

# Add metadata
physeq <- merge_phyloseq(physeq, sample_data(metadata_table[metadata_table$Run %in% sample_names(physeq),]))

# Keep only healthy and IBS samples
physeq <- subset_samples(physeq, host_disease=="IBS" | host_disease=="Healthy")
physeq <- prune_taxa(taxa_sums(physeq) != 0, physeq)
```



### 2. Save to disk

```{r, eval=FALSE}
# Save to disk
saveRDS(out2, "~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019//01_Dada2-Hugerth/out2.rds")
saveRDS(errF, "~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/01_Dada2-Hugerth/errF.rds")
saveRDS(errR, "~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/01_Dada2-Hugerth/errR.rds")
saveRDS(dadaFs, "~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/01_Dada2-Hugerth/infered_seq_F.rds")
saveRDS(dadaRs, "~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/01_Dada2-Hugerth/infered_seq_R.rds")
saveRDS(mergers, "~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/01_Dada2-Hugerth/mergers.rds")

saveRDS(otu_table(physeq), "~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/01_Dada2-Hugerth/ASVtable_final.rds") # ASV table
saveRDS(tax_table(physeq), "~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/01_Dada2-Hugerth/taxa_final.rds") # taxa table
write.csv(sample_data(physeq), "~/Projects/IBS_Meta-analysis_16S/data/analysis-individual/Hugerth-2019/HugerthMetadata.csv") # taxa table

saveRDS(physeq, "~/Projects/IBS_Meta-analysis_16S/phyloseq-objects/physeq_hugerth.rds") # phyloseq object


# Save nb of reads per sample before / after dada2 pipeline
nb.reads <- readRDS("~/Projects/IBS_Meta-analysis_16S/data/nb_reads.rds")
nb.reads[1:747, 'Hugerth_before'] <- fastqq(fnFs)@nReads
nb.reads[1:744, 'Hugerth_after'] <- rowSums(seqtable.nochim)
median(na.omit(nb.reads$Hugerth_before)) # sanity check
median(na.omit(nb.reads$Hugerth_after)) # sanity check
saveRDS(nb.reads, "~/Projects/IBS_Meta-analysis_16S/data/nb_reads.rds")
```


### 3. Quick peek at data analysis

```{r plot, fig.width = 10, fig.height = 4}
# Absolute abundance
plot_bar(physeq, fill = "Phylum")+ facet_wrap("host_disease", scales="free") + theme(axis.text.x = element_blank())

# Relative abundance for Phylum
phylum.table <- physeq %>%
  tax_glom(taxrank = "Phylum") %>%                     # agglomerate at phylum level
  transform_sample_counts(function(x) {x/sum(x)} ) %>% # Transform to rel. abundance
  psmelt()                                             # Melt to long format

ggplot(phylum.table, aes(x = reorder(Sample, Sample, function(x) mean(phylum.table[Sample == x & Phylum == 'Bacteroidota', 'Abundance'])),
                         y = Abundance, fill = Phylum))+
  facet_wrap(~ host_disease, scales = "free") + # scales = "free" removes empty lines
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(size = 2, angle = -90))+
  labs(x = "Samples", y = "Relative abundance")
```
