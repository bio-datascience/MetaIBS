---
title: "Dada2_Zeber2016"
params:
  date: "!r Sys.Date()"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
---

```{r, include=FALSE}
knitr::opts_chunk$set(fig.width = 3.5, fig.height = 3, warning=FALSE, message=FALSE,
                      root.dir = "~/Projects/MetaIBS/scripts/analysis-individual/Zeber-2016")
```




***********
# 1. IMPORT
***********

```{r library-import}
library(dada2)
packageVersion("dada2") # check dada2 version

library(Biostrings)
library(ShortRead)
library(seqTools) # per base sequence content
library(phyloseq)
library(ggplot2)
library(data.table)
library(plyr)
library(dplyr)
library(qckitfastq) # per base sequence content
library(stringr)

# ROOT DIRECTORY (to modify on your computer)
path.root <- "~/Projects/MetaIBS"
path.zeber  <- file.path(path.root, "scripts/analysis-individual/Zeber-2016")
path.data   <- file.path(path.root, "data/analysis-individual/Zeber-2016")
```


```{r html-import, echo = FALSE}
#_____________________________________________________________________
#____________ THIS IS ONLY FOR (quicker) PDF/HTML OUTPUT __________________
#_____________________________________________________________________

raw_stats     <- readRDS(file.path(path.data, "01_Dada2-Zeber/raw_stats.rds")) # seq depth
out           <- readRDS(file.path(path.data, "01_Dada2-Zeber/out.rds")) # out process (QC)
filt2_samples <- sort(list.files(file.path(path.data, "filtered2"), pattern=".fastq.gz", full.names = TRUE)) # quality-filtered reads
err           <- readRDS(file.path(path.data, "01_Dada2-Zeber/error_rates.rds")) # error rates
err_new       <- readRDS(file.path(path.data, "01_Dada2-Zeber/error_rates_modified.rds")) # modified error rates
seq_infered   <- readRDS(file.path(path.data, "01_Dada2-Zeber/seq_infered.rds")) # inferred ASVs
taxa          <- readRDS(file.path(path.data, "01_Dada2-Zeber/taxa_zeber.rds")) # taxa
physeq        <- readRDS(file.path(path.root, "data/analysis-individual/CLUSTER/PhyloTree/input/physeq_zeber.rds")) # phyloseq object without phylogenetic tree
```




******************
# 2. QUALITY CHECK
******************

### 2.1. Fastq quality profiles
First, we import the fastq files containing the raw reads. The samples were downloaded from the ENA database with the accession number PRJEB11252.

```{r quality-check, echo=TRUE, results="hide"}
# Save the path to the directory containing the fastq zipped files
path.fastq <- file.path(path.data, "raw_fastq")
# list.files(path.fastq) # check we are in the right directory

# fastq filenames have format: SAMPLENAME.fastq.gz
# Saves the whole directory path to each file name
FNs <- sort(list.files(path.fastq, pattern=".fastq.gz", full.names = TRUE))
show(FNs[1:5])

# Extract sample names, assuming filenames have format: SAMPLENAME.fastq.gz
sample.names <- sapply(strsplit(basename(FNs), ".fastq.gz"), `[`, 1)
show(sample.names[1:5])

# Look at quality of all files
for (i in 1:4){
  show(plotQualityProfile(FNs[i]))
}

# Look at nb of reads per sample
# raw_stats <- data.frame('sample' = sample.names,
#                         'reads' = fastqq(FNs)@nReads)
# min(raw_stats$reads)
# max(raw_stats$reads)
# mean(raw_stats$reads)
```

We will have a quick peak at the per base sequence content of the reads in some samples, to make sure there is no anomaly (i.e. all reads having the same sequence).

```{r per-base-seq-content, fig.width = 6, fig.height = 3}
# Look at per base sequence content
fseq <- seqTools::fastqq(FNs[1])
rc <- read_content(fseq)
plot_read_content(rc) + labs(title = "Per base sequence content")
```


### 2.2. Look for primers
The authors used the Ion 16S Metagenomics Kit, which contains two mixes of primers with unknown sequences (V2-4-8 and V3-6,7-9). There is no consensus on how to process the raw 16S data generated with this Kit.
The reads were filtered following recommendations from QIIME forum: https://forum.qiime2.org/t/possible-analysis-pipeline-for-ion-torrent-16s-metagenomics-kit-data-in-qiime2/13476




********************
# 3. FILTER AND TRIM
********************

### 3.1. Primer removal
Primer removal is not possible.


### 3.2. Quality filtering
We perform a quality filtering of the reads.

```{r filter-trim, eval = FALSE, echo = TRUE}
# Place filtered files in a filtered/ subdirectory
filt2_samples <- file.path(path.data, "filtered2", paste0(sample.names, "_filt2.fastq.gz"))
# Assign names for the filtered fastq.gz files
names(filt2_samples) <- sample.names

# Filter
out <- filterAndTrim(fwd = FNs, filt = filt2_samples,
                     trimLeft = 15, # as recommanded on QIIME forum
                     maxEE=3, # reads with more than 3 expected errors (sum(10e(-Q/10))) are discarded
                     truncQ=10, # Truncate reads at the first instance of a quality score less than or equal to truncQ.
                     minLen=150, # Discard reads shorter than 150 bp.
                     compress=TRUE,
                     multithread=TRUE,
                     verbose=TRUE)
```

Let's look at the output filtered fastq files as sanity check.

```{r quality-filter-check}
out[1:4,] # show how many reads were filtered in each file

# Look at quality profile of filtered files
for (i in 1:4){
  show(plotQualityProfile(filt2_samples[i]))
}
```




************************
# 4. CONSTRUCT ASV TABLE
************************

### 4.1. Learn error rates
Now we will build the parametric error model, to be able to infer amplicon sequence variants (ASVs) later on.

```{r error-rate-estimate, eval = FALSE}
set.seed(123)
err <- learnErrors(filt2_samples, multithread=TRUE, randomize=TRUE, verbose = 1)
```

The error rates for each possible transition (A→C, A→G, …) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score.

```{r plot-errors, fig.height = 5, fig.width = 5}
plotErrors(err, nominalQ = TRUE)
```

As the error rate model doesn't fit well the observations for high quality scores, we will manually modify it to better fit (as recommended in https://github.com/benjjneb/dada2/issues/1019)
```{r error-rate-modify, eval = FALSE}
# Modify error rate model
err_new <- err

err_new$err_out['A2C','40':'45'] <- 0.003
err_new$err_out['A2G','40':'45'] <- 0.012
err_new$err_out['A2T','39':'45'] <- 0.003
err_new$err_out['C2A','40':'45'] <- 0.003
err_new$err_out['C2G','39':'45'] <- 0.003
err_new$err_out['C2T','39':'45'] <- 0.007
err_new$err_out['G2A','39':'45'] <- 0.0075
err_new$err_out['G2C','39':'45'] <- 0.0025
err_new$err_out['G2T','39':'45'] <- 0.002
err_new$err_out['T2A','39':'45'] <- 0.004
err_new$err_out['T2C','40':'45'] <- 0.01
err_new$err_out['T2G','40':'45'] <- 0.004
```

```{r plot-new-errors, fig.height = 5, fig.width = 5}
# Check modified error rate model
plotErrors(err_new, nominalQ = TRUE)
```


### 4.2. Infer sample composition
The _dada()_ algorithm infers sequence variants based on estimated errors (previous step). Firstly, we de-replicate the reads in each sample, to reduce the computation time.
De-replication is a common step in almost all modern ASV inference (or OTU picking) pipelines, but a unique feature of derepFastq is that it maintains a summary of the quality information for each dereplicated sequence in $quals.

```{r infer-sample-composition, eval = FALSE}
# Prepare empty vector for the infered sequences
seq_infered <- vector("list", length(sample.names))
names(seq_infered) <- sample.names

# Iterate through the 90 samples
for(sampl in sample.names) {
  cat("Processing:", sampl, "\n")
  derep <- derepFastq(filt2_samples[[sampl]]) # dereplicate the reads in the sample
  seq_infered[[sampl]] <- dada(derep, err=err_new, multithread=TRUE, # default parameters
                               HOMOPOLYMER_GAP_PENALTY=-1, BAND_SIZE=32) # parameters for Ion Torrent recommended
}
```

```{r}
# Inspect the infered sequence variants from samples 1:5 (with modified error rate model)
for (i in 1:5){
  print(seq_infered[[i]])
  print("________________")
}
```


### 4.3. Construct ASV table
We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.

```{r build-seq-table, fig.height=3, fig.width=4}
# Make sequence table from the infered sequence variants
seqtable <- makeSequenceTable(seq_infered)

# We should have 90 samples (90 rows)
dim(seqtable)

# Inspect distribution of sequence lengths
hist(nchar(getSequences(seqtable)), breaks = 100, xlab = "ASV length", ylab = "Number of ASVs", main="")
```


### 4.4. Remove chimeras
The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

```{r remove-chimeras}
seqtable.nochim <- removeBimeraDenovo(seqtable, method="consensus", multithread=TRUE, verbose=TRUE)

# Check how many sequence variants we have after removing chimeras
dim(seqtable.nochim)

# Check how many reads we have after removing chimeras (we should keep the vast majority of the reads, like > 80%)
sum(seqtable.nochim)/sum(seqtable)
```




*****************************************
# 5. LOOK AT READS COUNT THROUGH PIPELINE
*****************************************

Sanity check before assigning taxonomy.

```{r reads-filtered-sanity-check}
# Function that counts nb of reads
getN <- function(x) sum(getUniques(x))

# Table that will count number of reads for each process of interest (input reads, filtered reads, denoised reads, non chimera reads)
track <- cbind(out, sapply(seq_infered, getN), rowSums(seqtable.nochim),
               lapply(rowSums(seqtable.nochim)*100/out[,1], as.integer))

# Assign column and row names
colnames(track) <- c("input", "quality-filt", "denoised", "nonchim", "%input->output")
rownames(track) <- sample.names

# Show final table: for each row/sample, we have shown the initial number of reads, filtered reads, denoised reads, and non chimera reads
track
```

```{r seqdepth, echo=FALSE, eval=FALSE, include=FALSE}
# Save nb of reads per sample before / after dada2 pipeline
nb.reads <- data.frame("after"=rowSums(seqtable.nochim)) %>%
  mutate("Run"=names(rowSums(seqtable.nochim))) %>%
  right_join(raw_stats %>% dplyr::rename(Run=sample, before=reads),
             by="Run") %>%
  relocate(before) %>%
  relocate(Run) %>%
  mutate(after = nafill(after, fill=0),
         "dataset" = "Zeber")

saveRDS(nb.reads, file.path(path.root, "data/analysis-combined/06_QCplot/nbreads_zeber.rds")
```




********************
# 6. TAXONOMIC TABLE
********************

Extensions: The dada2 package also implements a method to make species level assignments based on exact matching between ASVs and sequenced reference strains. Recent analysis suggests that exact matching (or 100% identity) is the only appropriate way to assign species to 16S gene fragments. Currently, species-assignment training fastas are available for the Silva and RDP 16S databases. To follow the optional species addition step, download the silva_species_assignment_v132.fa.gz file, and place it in the directory with the fastq files.

```{r assign-taxonomy-on-cluster, eval=FALSE, echo=FALSE, include=FALSE}
# In case you prefer to assign taxonomy on a computer cluster (in other words run this chunk on a computer cluster), you can export the ASV table
# saveRDS(seqtable.nochim, file.path(path.root, "data/analysis-individual/CLUSTER/taxonomy/input/seqtablenochim_zeber.rds"))

# then you can :
# 1. copy the "data/analysis-individual/CLUSTER/taxonomy/" directory to your cluster (where there is the input data & R scripts to assign taxonomy)
# 2. run the R scripts on the cluster (they will output the taxonomic tables in the "output/" subdirectory)
# 3. copy the "taxonomy/output/" subdirectory back to your personal computer
# 4. import the taxonomic table below (in that case you don't need to run the next code chunk "assign-taxonomy"):
# taxa <- readRDS(file.path(path.root, "data/analysis-individual/CLUSTER/taxonomy/output/taxa_zeber.rds"))
```

```{r assign-taxonomy, eval = FALSE}
path.silva <- file.path(path.root, "data/analysis-individual/CLUSTER/taxonomy/silva-taxonomic-ref")

# Assign taxonomy (with silva v138)
set.seed(123)
taxa <- assignTaxonomy(seqtable.nochim, file.path(path.silva, "silva_nr99_v138.1_train_set.fa.gz"),
                       tryRC = TRUE, # try reverse complement of the sequences
                       multithread=TRUE, verbose = TRUE)

# Add species assignment
set.seed(123)
taxa <- addSpecies(taxa, file.path(path.silva, "silva_species_assignment_v138.1.fa.gz"))
```


```{r taxa-sanity-check}
# Check how the taxonomy table looks like
taxa.print <- taxa
rownames(taxa.print) <- NULL # Removing sequence rownames for display only
head(taxa.print)

table(taxa.print[,1], useNA="ifany") # Show the different kingdoms (should be only bacteria)
table(taxa.print[,2], useNA="ifany") # Show the different phyla
```




***********************
# 7. LAST PREPROCESSING
***********************

We will remove any sample with less than 500 reads from further analysis, and also any ASVs with unassigned phyla.

### 7.1. Create phyloseq object
The preprocessing will be easier to do with ASV, taxonomic and metadata tables combined in a phyloseq object.

```{r phyloseq-object, eval=FALSE}
# Import metadata
metadata_table <- read.csv(file.path(path.data, "00_Metadata-Zeber/Metadata-Zeber.csv"), row.names = 1)

#_________________________
# Create phyloseq object
physeq <- phyloseq(otu_table(seqtable.nochim, taxa_are_rows=FALSE), # by default, in otu_table the sequence variants are in rows
                  sample_data(metadata_table), 
                  tax_table(taxa))

# Remove taxa that are eukaryota, or have unassigned Phyla
physeq <- subset_taxa(physeq, Kingdom != "Eukaryota")
physeq <- subset_taxa(physeq, !is.na(Phylum))
# Remove samples with less than 500 reads
physeq <- prune_samples(sample_sums(physeq)>=500, physeq)
# No sample was deleted, so we don't need to remove taxa present in low-count samples
# physeq <- prune_taxa(taxa_sums(physeq)>0, physeq)
```


### 7.2. Quick peek at data analysis
```{r plot, fig.width = 10, fig.height = 4}
# Relative abundance for Phylum
phylum.table <- physeq %>%
  tax_glom(taxrank = "Phylum") %>%                     # agglomerate at phylum level
  transform_sample_counts(function(x) {x/sum(x)} ) %>% # Transform to rel. abundance
  psmelt()                                             # Melt to long format

ggplot(phylum.table, aes(x = reorder(Sample, Sample, function(x) mean(phylum.table[Sample == x & Phylum == 'Firmicutes', 'Abundance'])),
                         y = Abundance, fill = Phylum))+
  facet_wrap(~ host_disease, scales = "free") + # scales = "free" removes empty lines
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(size = 5, angle = -90))+
  labs(x = "Samples", y = "Relative abundance")
```


### 7.3. Save to disk
```{r save, eval=FALSE}
# Save to disk
saveRDS(raw_stats,   file.path(path.data, "01_Dada2-Zeber/raw_stats.rds"))
saveRDS(out,         file.path(path.data, "01_Dada2-Zeber/out.rds"))
saveRDS(err,         file.path(path.data, "01_Dada2-Zeber/error_rates.rds"))
saveRDS(err_new,     file.path(path.data, "01_Dada2-Zeber/error_rates_modified.rds"))
saveRDS(seq_infered, file.path(path.data, "01_Dada2-Zeber/seq_infered.rds"))

# Taxa & Phyloseq object
saveRDS(taxa,   file.path(path.data, "01_Dada2-Zeber/taxa_zeber.rds"))
saveRDS(physeq, file.path(path.root, "data/analysis-individual/CLUSTER/PhyloTree/input/physeq_zeber.rds"))
saveRDS(physeq, file.path(path.root, "data/phyloseq-objects/phyloseq-without-phylotree/physeq_zeber.rds"))
```




******************
# 8. SESSION INFO
******************

```{r session-info}
sessionInfo()
```