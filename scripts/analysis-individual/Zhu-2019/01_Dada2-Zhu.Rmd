---
title: "Dada2_Zhu2019"
params:
  date: "!r Sys.Date()"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
---

```{r, include=FALSE}
knitr::opts_chunk$set(fig.width = 3.5, fig.height = 3, warning=FALSE, message=FALSE,
                      root.dir = "~/Projects/MetaIBS/scripts/analysis-individual/Zhu-2019")
```




***********
# 1. IMPORT
***********

```{r library-import}
library(dada2)
packageVersion("dada2") # check dada2 version

library(Biostrings)
library(ShortRead)
library(seqTools) # per base sequence content
library(phyloseq)
library(ggplot2)
library(data.table)
library(plyr)
library(dplyr)
library(qckitfastq) # per base sequence content
library(stringr)

# ROOT DIRECTORY (to modify on your computer)
path.root <- "~/Projects/MetaIBS"
path.zhu  <- file.path(path.root, "scripts/analysis-individual/Zhu-2019")
path.data <- file.path(path.root, "data/analysis-individual/Zhu-2019")
```


```{r html-import, echo = FALSE}
#__________________________________________________________________________
#____________ THIS IS ONLY FOR (quicker) PDF/HTML OUTPUT __________________
#__________________________________________________________________________

raw_stats         <- readRDS(file.path(path.data, "01_Dada2-Zhu/raw_stats.rds")) # seq depth
FWDpos.FWDread    <- readRDS(file.path(path.data, "01_Dada2-Zhu/forwardreads_primerposition.rds"))
FWDpos.REVread    <- readRDS(file.path(path.data, "01_Dada2-Zhu/reversereads_primerposition.rds"))
out2              <- readRDS(file.path(path.data, "01_Dada2-Zhu/out2.rds")) # out process (QC)
FWD.filt2_samples <- sort(list.files(file.path(path.data, "filtered2"), pattern="_1_filt2.fastq.gz", full.names = TRUE)) # quality-filtered reads (FWD)
REV.filt2_samples <- sort(list.files(file.path(path.data, "filtered2"), pattern="_2_filt2.fastq.gz", full.names = TRUE)) # quality-filtered reads (REV)
errF              <- readRDS(file.path(path.data, "01_Dada2-Zhu/errF.rds")) # error rates F
errR              <- readRDS(file.path(path.data, "01_Dada2-Zhu/errR.rds")) # error rates R
dadaFs            <- readRDS(file.path(path.data, "01_Dada2-Zhu/infered_seq_F.rds")) # inferred ASVs F
dadaRs            <- readRDS(file.path(path.data, "01_Dada2-Zhu/infered_seq_R.rds")) # inferred ASVs R
mergers           <- readRDS(file.path(path.data, "01_Dada2-Zhu/mergers.rds")) # mergers
taxa              <- readRDS(file.path(path.data, "01_Dada2-Zhu/taxa_zhu_longASV.rds")) # taxa
taxa.small        <- readRDS(file.path(path.data, "01_Dada2-Zhu/taxa_zhu_shortASV.rds")) # taxa from trimmed ASVs
physeq            <- readRDS(file.path(path.data, "01_Dada2-Zhu/physeq_zhu_longASV.rds")) # phyloseq object without phylogenetic tree
physeq.small      <- readRDS(file.path(path.root, "data/analysis-individual/CLUSTER/PhyloTree/input/physeq_zhu.rds")) # phyloseq object without phylogenetic tree (and with trimmed ASVs)
```




******************
# 2. QUALITY CHECK
******************

### 2.1. Fastq quality profiles
First, we import the fastq files containing the raw reads. The samples were downloaded from the ENA database with the accession number PRJNA566284.

```{r, quality-check, echo=TRUE, results="hide"}
# Save the path to the directory containing the fastq zipped files
path.fastq <- file.path(path.data, "raw_fastq")
# list.files(path.fastq) # check we are in the right directory

# fastq filenames have format: SAMPLENAME.fastq.gz
# Saves the whole directory path to each file name
fnFs <- sort(list.files(path.fastq, pattern="_1.fastq.gz", full.names = TRUE)) # FWD reads
fnRs <- sort(list.files(path.fastq, pattern="_2.fastq.gz", full.names = TRUE)) # REV reads
show(fnFs[1:5])
show(fnRs[1:5])

# Extract sample names, assuming filenames have format: SAMPLENAME.fastq.gz
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
show(sample.names) # saves only the file name (without the path)

# Look at quality of all files
for (i in 1:3){  # 1:length(fnFs)
  show(plotQualityProfile(fnFs[i]))
  show(plotQualityProfile(fnRs[i]))
}

# Look at nb of reads per sample
# raw_stats <- data.frame('sample' = sample.names,
#                         'reads' = fastqq(fnFs)@nReads)
# min(raw_stats$reads)
# max(raw_stats$reads)
# mean(raw_stats$reads)
```

We will have a quick peak at the per base sequence content of the reads in some samples, to make sure there is no anomaly (i.e. all reads having the same sequence).

```{r per-base-seq-content, fig.width = 6, fig.height = 3}
# Look at per base sequence content (forward read)
fseqF1 <- seqTools::fastqq(fnFs[10])
rcF1 <- read_content(fseqF1)
plot_read_content(rcF1) + labs(title = "Per base sequence content - Forward read")

fseqR1 <- seqTools::fastqq(fnRs[10])
rcR1 <- read_content(fseqR1)
plot_read_content(rcR1) + labs(title = "Per base sequence content - Reverse read")
```


### 2.2. Look for primers
Now, we will look whether the reads still contain the primers. Primer sequences are given in the methods section of the paper.

```{r primer-check, eval = TRUE, message=FALSE}
# V4
FWD <- "GTGCCAGCMGCCGCGGTAA"  # U515 forward primer sequence
REV <- "GGACTACHVGGGTWTCTAAT" # E786 reverse primer sequence (in reality, it's the sequence of 5'-806-786-3')

# Function that, from the primer sequence, will return all combinations possible (complement, reverse complement, ...)
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}

# Get all combinations of the primer sequences
FWD.orients <- allOrients(FWD) # U515
REV.orients <- allOrients(REV) # E786
FWD.orients # sanity check
REV.orients

# Function that counts number of reads in which a sequence is found
primerHits <- function(primer, fn) {
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE, max.mismatch = 2)
    return(sum(nhits > 0))
}

# Get a table to know how many times the U515 and E786 primers are found in the reads of each sample
for (i in 6:9){
  cat("SAMPLE", sample.names[i], "with total number of", raw_stats[i,'reads'], "reads\n\n")
  x <- rbind(ForwardRead.FWDPrimer = sapply(FWD.orients, primerHits, fn = fnFs[[i]]),
             ForwardRead.REVPrimer = sapply(REV.orients, primerHits, fn = fnFs[[i]]),
             ReverseRead.FWDPrimer = sapply(FWD.orients, primerHits, fn = fnRs[[i]]), 
             ReverseRead.REVPrimer = sapply(REV.orients, primerHits, fn = fnRs[[i]]))
  print(x)
  cat("\n____________________________________________\n\n")
}
```

Let's have a quick look at where primers are positioned in the forward/reverse reads
```{r primer-position, eval=FALSE, echo=TRUE}
# Function that gets position in which sequence is found
primerHitsPosition <- function(primer, fn){
  hits <- as.data.frame(vmatchPattern(primer, sread(readFastq(fn)), fixed = FALSE, max.mismatch = 2))
  hits <- hits[,c("group", "start")]
  colnames(hits) <- c("sample", "start")
  hits$sample <- sapply(strsplit(basename(fn), "_"), `[`, 1)
  hits$readslength <- seqTools::fastqq(fn)@maxSeqLen
  return(hits)
}

# Get position of primers in forward reads
FWDpos.FWDread <- data.frame()
for(i in 1:length(fnFs)){
  cat("SAMPLE", i)
  newF <- primerHitsPosition(FWD.orients["Forward"], fnFs[[i]])
  FWDpos.FWDread <- rbind(newF, FWDpos.FWDread)
}

# Get position of REV primers
FWDpos.REVread <- data.frame()
for(i in 1:length(fnRs)){
  cat("SAMPLE", i)
  newF <- primerHitsPosition(FWD.orients["RevComp"], fnRs[[i]])
  FWDpos.REVread <- rbind(newF, FWDpos.REVread)
}
# add metadata info on samples
metadata_table <- read.csv(file.path(path.data, "00_Metadata-Zhu/Metadata-Zhu.csv"), row.names=1)
FWDpos.REVread <- FWDpos.REVread %>%
  left_join(metadata_table[,c("Run", "host_disease")], by=join_by("sample"=="Run"))
```

```{r primer-position-plot, fig.width = 5, fig.height = 4}
# FORWARD READS
ggplot(FWDpos.FWDread, aes(x=start))+
  geom_density() +
  xlim(c(0,max(FWDpos.FWDread$readslength)))+
  labs(x="start position of primer", y="density of primers starting at x position", title="FORWARD READS")

# REVERSE READS
ggplot(FWDpos.REVread, aes(x=start))+
  geom_density() +
  facet_wrap(~sample)+ # weird pattern...
  xlim(c(0,max(FWDpos.REVread$readslength)))+
  labs(x="start position of primer", y="density of primers starting at x position", title="REVERSE READS")

ggplot(FWDpos.REVread, aes(x=start))+
  geom_density() +
  facet_wrap(~host_disease)+ # seems like it's determined by host_disease...
  xlim(c(0,max(FWDpos.REVread$readslength)))+
  labs(x="start position of primer", y="density of primers starting at x position", title="REVERSE READS")
```



********************
# 3. FILTER AND TRIM
********************

### 3.1. Primer removal
The forward reads indeed contain the forward primer (in the middle of the reads), and the reverse reads contain the revcomp of the forward primer (which makes sense, it shows the forward primer is found at the *end* of the reverse reads). But only half the samples contain a primer in the reverse reads (seems like it's the healthy samples).
Ideally, to follow the standardized pipeline, we should (1) filter out reads not containing any primer and (2) trim the primers. However, we would lose all of our IBS samples in step 1. Thus, we will simply perform a quality filtering of the reads (step 2).


### 3.2. Quality filtering
Then, we perform a quality filtering of the reads.

```{r filter-trim, eval = FALSE, echo = TRUE}
# Place filtered files in a filtered/ subdirectory
FWD.filt2_samples <- file.path(path.data, "filtered2", paste0(sample.names, "_1_filt2.fastq.gz")) # FWD reads
REV.filt2_samples <- file.path(path.data, "filtered2", paste0(sample.names, "_2_filt2.fastq.gz")) # REV reads
# Assign names for the filtered fastq.gz files
names(FWD.filt2_samples) <- sample.names
names(REV.filt2_samples) <- sample.names

# Filter
out2 <- filterAndTrim(fwd = fnFs, filt = FWD.filt2_samples,
                      rev = fnRs, filt.rev = REV.filt2_samples,
                      maxEE=3, # reads with more than 3 expected errors (sum(10e(-Q/10))) are discarded
                      truncQ=10, # Truncate reads at the first instance of a quality score less than or equal to truncQ.
                      minLen = 150, # Discard reads shorter than 150 bp. This is done after trimming and truncation.
                      compress=TRUE, multithread = TRUE, verbose=TRUE)

```

Let's look at the output filtered fastq files as sanity check.

```{r filter-check}
out2[1:6,] # show how many reads were filtered in each file

# Look at quality profile of all filtered files
for (i in 1:3){
  show(plotQualityProfile(FWD.filt2_samples[i]))
  show(plotQualityProfile(REV.filt2_samples[i]))
}
```




************************
# 4. CONSTRUCT ASV TABLE
************************

### 4.1. Learn error rates
Now we will build the parametric error model, to be able to infer amplicon sequence variants (ASVs) later on.

```{r error-rate-estimate, eval = FALSE}
set.seed(123)
errF <- learnErrors(FWD.filt2_samples, multithread=TRUE, randomize=TRUE, verbose = 1)
set.seed(123)
errR <- learnErrors(REV.filt2_samples, multithread=TRUE, randomize=TRUE, verbose = 1)
```

The error rates for each possible transition (A→C, A→G, …) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected.

```{r plot-errors, fig.height=5, fig.width=5}
plotErrors(errF, nominalQ = TRUE) # Forward reads
plotErrors(errR, nominalQ = TRUE) # Reverse reads
```


### 4.2. Infer sample composition
The _dada()_ algorithm infers sequence variants based on estimated errors (previous step). Firstly, we de-replicate the reads in each sample, to reduce the computation time.
De-replication is a common step in almost all modern ASV inference (or OTU picking) pipelines, but a unique feature of derepFastq is that it maintains a summary of the quality information for each dereplicated sequence in $quals.

```{r infer-sample-composition, eval = FALSE}
# Dereplicate the reads in the sample
derepF <- derepFastq(FWD.filt2_samples) # forward
derepR <- derepFastq(REV.filt2_samples) # reverse

# Infer sequence variants
dadaFs <- dada(derepF, err=errF, multithread=TRUE) # forward
dadaRs <- dada(derepR, err=errR, multithread=TRUE) # reverse
```

```{r}
# Inspect the infered sequence variants from sample 1:3
for (i in 1:3){
  print(dadaFs[[i]])
  print(dadaRs[[i]])
  print("________________")
}
```


### 4.3. Merge paired-end reads
We now need to merge paired reads.

```{r merge-paired-end, eval = FALSE}
mergers <- mergePairs(dadaFs, derepF, dadaRs, derepR, verbose=TRUE)
```

```{r}
head(mergers[[1]])
```


### 4.4. Construct ASV table
We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.

```{r build-seq-table, fig.height=3, fig.width=4}
# Make sequence table from the infered sequence variants
seqtable <- makeSequenceTable(mergers)

# We should have 29 samples (29 rows)
dim(seqtable)

# Inspect distribution of sequence lengths
hist(nchar(getSequences(seqtable)), breaks = 100, xlab = "ASV length", ylab = "Number of ASVs", main="")
```

The V4 region should be ~250bp, and our ASVs are rather ~400bp long. Considering that the forward primer was found in the middle of the forward reads, it is possible that we have much longer ASVs than we should (pre-V4 region + V4 region).
Let's try trimming the ASVs until the end of the forward primer.

```{r cut-asvs}
# Get the ASVs
asvs <- getSequences(seqtable)

# Trim the ASVs when they contain the forward primer.
# FWD primer: GTGCCAGCMGCCGCGGTAA (M: A or C)
new.asvs <- sub(".*GTGCCAGCAGCCGCGGTAA|.*GTGCCAGCCGCCGCGGTAA", "", getSequences(seqtable))
length(new.asvs) == length(asvs) # sanity check

# Let's check the length of our new asvs
hist(nchar(new.asvs), breaks = 100, xlab = "ASV length", ylab = "Number of ASVs", main="")

# Put these new ASVs in a separate seqtable
seqtable.new <- seqtable
colnames(seqtable.new) <- new.asvs

# Sanity check
# for (i in 1:length(asvs)){
#   test <- str_detect(string = getSequences(seqtable)[i],
#                      pattern = getSequences(seqtable.new)[i])
#   if(test==FALSE){print('FALSE')}
# }

# Is there any duplicate ASVs now?
table(duplicated(new.asvs))
table(duplicated(asvs))

# Create table to have corresponding ASVs pre/post-cut (to compare taxonomy assignment later on)
asv.df <- data.frame("asv" = paste0("ASV", 1:length(asvs)),
                     "pre_cut" = asvs,
                     "post_cut" = new.asvs)

# We will merge columns with same ASV and sum their counts
seqtable.small <- sapply(unique(colnames(seqtable.new)),
                         function(i) as.integer(rowSums(as.data.frame(seqtable.new[,colnames(seqtable.new) == i]))))
rownames(seqtable.small) <- rownames(seqtable.new) # add sample names as rownames

# sanity checks
ncol(seqtable.small) == length(unique(new.asvs))
rowSums(seqtable.new) == rowSums(seqtable.small)
sum(seqtable.new) == sum(seqtable.small)

# Check the ASVs are same sequence length, but we should have less of them
hist(nchar(getSequences(seqtable.small)), breaks = 100, xlab = "ASV length", ylab = "Number of ASVs", main="")

# Sequences should be between 515F - 806R, so around ~250bp (removing the primers lengths).
# Remove any sequence variant outside 200:300bp
seqtable.small.new <- seqtable.small[,nchar(colnames(seqtable.small)) %in% 200:300]
dim(seqtable.small.new)
hist(nchar(getSequences(seqtable.small.new)), breaks = 100, xlab = "ASV length", ylab = "Number of ASVs", main="")
# check we haven't lost too many counts
sum(seqtable.small.new)/sum(seqtable.small)
```


### 4.5. Remove chimeras
The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

```{r remove-chimeras}
seqtable.nochim <- removeBimeraDenovo(seqtable, method="consensus", multithread=TRUE, verbose=TRUE)

# Check how many sequence variants we have after removing chimeras
dim(seqtable.nochim)

# Check how many reads we have after removing chimeras (we should keep the vast majority of the reads, like > 80%)
sum(seqtable.nochim)/sum(seqtable)

# Same for the table with ASVs where I cut the FWD primer
seqtable.small.nochim <- removeBimeraDenovo(seqtable.small.new, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtable.small.nochim)
sum(seqtable.small.nochim)/sum(seqtable.small.new)
```




*****************************************
# 5. LOOK AT READS COUNT THROUGH PIPELINE
*****************************************

Sanity check before assigning taxonomy.

```{r reads-filtered-sanity-check}
# Function that counts nb of reads
getN <- function(x) sum(getUniques(x))

# Table that will count number of reads for each process of interest (input reads, filtered reads, denoised reads, non chimera reads)
track <- cbind(out2,
               sapply(dadaFs, getN),
               sapply(dadaRs, getN),
               sapply(mergers, getN),
               rowSums(seqtable.small.nochim),
               lapply(rowSums(seqtable.small.nochim)*100/out2[,1], as.integer))

# Assign column and row names
colnames(track) <- c("input", "quality-filt", "denoisedF", "denoisedR", 'merged', 'nonchim', "%input->output")
rownames(track) <- sample.names

# Show final table: for each row/sample, we have shown the initial number of reads, filtered reads, denoised reads, and non chimera reads
track
```

```{r seqdepth, echo=FALSE, eval=FALSE, include=FALSE}
# Save nb of reads per sample before / after dada2 pipeline
nb.reads <- data.frame("after"=rowSums(seqtable.small.nochim)) %>%
  mutate("Run"=names(rowSums(seqtable.small.nochim))) %>%
  right_join(raw_stats %>% dplyr::rename(Run=sample, before=reads),
             by="Run") %>%
  relocate(before) %>%
  relocate(Run) %>%
  mutate(after = nafill(after, fill=0),
         "dataset" = "Zhu")

saveRDS(nb.reads, file.path(path.root, "data/analysis-combined/06_QCplot/nbreads_zhu.rds"))
```




********************
# 6. TAXONOMIC TABLE
********************

### 6.1. Assign taxonomy
Extensions: The dada2 package also implements a method to make species level assignments based on exact matching between ASVs and sequenced reference strains. Recent analysis suggests that exact matching (or 100% identity) is the only appropriate way to assign species to 16S gene fragments. Currently, species-assignment training fastas are available for the Silva and RDP 16S databases. To follow the optional species addition step, download the silva_species_assignment_v132.fa.gz file, and place it in the directory with the fastq files.

```{r assign-taxonomy-on-cluster, eval=FALSE, echo=FALSE, include=FALSE}
# In case you prefer to assign taxonomy on a computer cluster (in other words run this chunk on a computer cluster), you can export the ASV table
# saveRDS(seqtable.small.nochim, file.path(path.root, "data/analysis-individual/CLUSTER/taxonomy/input/seqtablenochim_zhu.rds"))

# then you can :
# 1. copy the "data/analysis-individual/CLUSTER/taxonomy/" directory to your cluster (where there is the input data & R scripts to assign taxonomy)
# 2. run the R scripts on the cluster (they will output the taxonomic tables in the "output/" subdirectory)
# 3. copy the "taxonomy/output/" subdirectory back to your personal computer
# 4. import the taxonomic table below (in that case you don't need to run the next code chunk "assign-taxonomy"):
# taxa <- readRDS(file.path(path.root, "data/analysis-individual/CLUSTER/taxonomy/output/taxa_zhu.rds"))
```

```{r assign-taxonomy, eval = FALSE, echo = TRUE}
path.silva <- file.path(path.root, "data/analysis-individual/CLUSTER/taxonomy/silva-taxonomic-ref")

# Assign taxonomy (with silva v138)
set.seed(123)
taxa <- assignTaxonomy(seqtable.nochim, file.path(path.silva, "silva_nr99_v138.1_train_set.fa.gz"),
                       tryRC = TRUE, # try reverse complement of the sequences
                       multithread=TRUE, verbose = TRUE)
# Add species assignment
set.seed(123)
taxa <- addSpecies(taxa, file.path(path.silva, "silva_species_assignment_v138.1.fa.gz"))


# Same with our optimized seqtable
set.seed(123)
taxa.small <- assignTaxonomy(seqtable.small.nochim, file.path(path.silva, "silva_nr99_v138.1_train_set.fa.gz"),
                            tryRC = TRUE, # try reverse complement of the sequences
                            multithread=TRUE, verbose = TRUE)
set.seed(123)
taxa.small <- addSpecies(taxa.small, file.path(path.silva, "silva_species_assignment_v138.1.fa.gz"))
```

```{r taxtable-check}
# Check how the taxonomy table looks like
taxa.print <- taxa
rownames(taxa.print) <- NULL # Removing sequence rownames for display only
head(taxa.print)

table(taxa.print[,1], useNA="ifany") # Show the different kingdoms
table(taxa.print[,2], useNA="ifany") # Show the different phyla
```

```{r taxtable-check2}
# Check how the taxonomy table looks like (where we trimmed the ASV sequences)
taxa.small.print <- taxa.small
rownames(taxa.small.print) <- NULL
head(taxa.small.print)

table(taxa.small.print[,1], useNA="ifany") # Show the different kingdoms
table(taxa.small.print[,2], useNA="ifany") # Show the different phyla
```



### 6.2. Compare taxonomic assignment with/without cutting ASVs
```{r compare-tax-assignment, fig.width = 6, fig.height = 6}
# taxa dataframes
taxa.df <- data.frame(taxa)
taxa.df$asv <- rownames(taxa.df)
taxa.small.df <- data.frame(taxa.small)
taxa.small.df$asv <- rownames(taxa.small.df)

# remove chimera from asv pre/post cut dataframe
asv.df.nochim <- asv.df[asv.df$pre_cut %in% rownames(taxa.df),]
colnames(asv.df.nochim) <- c("asvID", "asv_precut", "asv_postcut")
# Sanity checks
# table(duplicated(asv.df.nochim$asv_postcut))
# dim(asv.df.nochim) # should be 1057
# table(asv.df.nochim$asv_precut %in% rownames(taxa))
# table(asv.df.nochim$asv_postcut %in% rownames(taxa.small)) # 56 ASVs post-cut are not in the taxonomic table (probably because they are chimeras)

# Compare taxonomic assignment pre/post-cut (phylum level)
compare.df <- merge(asv.df.nochim, taxa.df[,c("Phylum", "asv")], by.x="asv_precut", by.y="asv") # pre-cut phylum
colnames(compare.df)[4] <- "Phylum_precut"
compare.df <- merge(x=compare.df, y=taxa.small.df[,c("Phylum", "asv")], by.x="asv_postcut", by.y="asv") # post-cut phylum
colnames(compare.df)[5] <- "Phylum_postcut"

# The 56 ASVs present in pre-cut but not in post-cut were removed (59 rows were deleted)
dim(compare.df)

# How many rows (ASVs) have different phylum?
table(compare.df$Phylum_precut == compare.df$Phylum_postcut, useNA="ifany") # 26 ASVs assigned to different phyla
phylum.all <- compare.df %>%
  select(asvID, Phylum_precut, Phylum_postcut) %>%
  dplyr::rename(precut=Phylum_precut, postcut=Phylum_postcut) %>%
  mutate(diff=ifelse(precut != postcut, TRUE, FALSE)) %>%
  tidyr::pivot_longer(!c(asvID, diff), names_to="cut", values_to="Phylum")
  

# Plot
library(ggalluvial)
ggplot(phylum.all,
       aes(x = cut, stratum = Phylum, alluvium = asvID,
           label = Phylum)) +
  geom_flow(aes(color=diff),stat = "alluvium", lode.guidance = "frontback") +
  geom_stratum(aes(fill=Phylum)) +
  scale_color_manual(values=c("lightgrey", "#de2d26", "lightgrey"))+
  scale_fill_brewer(type = "qual", palette = "Set3") +
  theme_bw()+
  labs(x="", y="#ASVs", title="ASV taxonomic assignment (with/without cutting)", color='Different?')
# ggsave(file.path(path.data, "01_Dada2-Zhu/pre-post-cut_taxAssignment_phylum.jpg"), width=10, height=6)


# *********************
# SAME AT GENUS LEVEL
compare.df.genus <- merge(asv.df.nochim, taxa.df[,c("Genus", "asv")], by.x="asv_precut", by.y="asv") # pre-cut genus
colnames(compare.df.genus)[4] <- "Genus_precut"
compare.df.genus <- merge(x=compare.df.genus, y=taxa.small.df[,c("Genus", "asv")], by.x="asv_postcut", by.y="asv") # post-cut genus
colnames(compare.df.genus)[5] <- "Genus_postcut"
dim(compare.df.genus)

# How many rows (ASVs) have different phylum?
table(compare.df.genus$Genus_precut == compare.df.genus$Genus_postcut, useNA="ifany") # 31 ASVs assigned to diff. genus, but many more NAs
genus.all <- compare.df.genus %>%
  select(asvID, Genus_precut, Genus_postcut) %>%
  dplyr::rename(precut=Genus_precut, postcut=Genus_postcut) %>%
  mutate(diff=ifelse(precut != postcut, TRUE, FALSE)) %>%
  tidyr::pivot_longer(!c(asvID, diff), names_to="cut", values_to="Genus")


# Plot
ggplot(genus.all,
       aes(x = cut, stratum = Genus, alluvium = asvID,
           label = Genus)) +
  geom_flow(aes(color=diff),stat = "alluvium", lode.guidance = "frontback") +
  geom_stratum(aes(fill=Genus)) +
  scale_color_manual(values=c("lightgrey", "#de2d26", "lightgrey"))+
  theme_bw()+
  theme(legend.text = element_text(size=3),
        legend.key.size = unit(0.1, "cm"))+
  labs(x="", y="#ASVs", title="ASV taxonomic assignment (with/without cutting)", color='Different?')+
  guides(fill=guide_legend(ncol=3))
# ggsave(file.path(path.data, "01_Dada2-Zhu/pre-post-cut_taxAssignment_genus.jpg"), width=10, height=6)
```




***********************
# 7. LAST PREPROCESSING
***********************

We will remove any sample with less than 500 reads from further analysis, and also any ASVs with unassigned phyla.

### 7.1. Create phyloseq object
The preprocessing will be easier to do with ASV, taxonomic and metadata tables combined in a phyloseq object.

```{r phyloseq-object, echo=TRUE, eval=FALSE}
#_________________________
# Import metadata
metadata_table <- read.csv(file.path(path.data, "00_Metadata-Zhu/Metadata-Zhu.csv"), row.names=1)

#_________________________
# Create phyloseq object
physeq <- phyloseq(otu_table(seqtable.nochim, taxa_are_rows=FALSE), # by default, in otu_table the ASVs are in rows
                  sample_data(metadata_table), 
                  tax_table(taxa))
physeq.small <- phyloseq(otu_table(seqtable.small.nochim, taxa_are_rows=FALSE),
                  sample_data(metadata_table), 
                  tax_table(taxa.small))

# Remove taxa that are eukaryota, or have unassigned Phyla
physeq <- subset_taxa(physeq, !is.na(Kingdom))
physeq <- subset_taxa(physeq, !is.na(Phylum))
# Remove samples with less than 500 reads
physeq <- prune_samples(sample_sums(physeq)>=500, physeq)
# No sample was deleted, so we don't need to remove taxa present in low-count samples
# physeq <- prune_taxa(taxa_sums(physeq)>0, physeq)

# Remove taxa that are eukaryota, or have unassigned Phyla
physeq.small <- subset_taxa(physeq.small, !is.na(Kingdom))
physeq.small <- subset_taxa(physeq.small, !is.na(Phylum))
# Remove samples with less than 500 reads
physeq.small <- prune_samples(sample_sums(physeq.small)>=500, physeq.small)
```


### 7.2. Quick peek at data analysis
```{r plot, fig.width = 10, fig.height = 5}
# Absolute abundance
# plot_bar(physeq, fill = "Phylum")+ facet_wrap("host_disease", scales="free_x") + theme(axis.text.x = element_blank())

# Relative abundance for Phylum
phylum.table <- physeq %>%
  tax_glom(taxrank = "Phylum") %>%                     # agglomerate at phylum level
  transform_sample_counts(function(x) {x/sum(x)} ) %>% # Transform to rel. abundance
  psmelt()                                             # Melt to long format

ggplot(phylum.table, aes(x = Sample, y = Abundance, fill = Phylum))+
  facet_wrap(~ host_disease, scales = "free") + # scales = "free" removes empty lines
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(size = 5, angle = -90))+
  labs(x = "Samples", y = "Relative abundance")
```

```{r plot-2, fig.width = 10, fig.height = 5}
# Absolute abundance
# plot_bar(physeq.small, fill = "Phylum")+ facet_wrap("host_disease", scales="free_x") + theme(axis.text.x = element_blank())

# Relative abundance for Phylum
phylum.table <- physeq.small %>%
  tax_glom(taxrank = "Phylum") %>%                     # agglomerate at phylum level
  transform_sample_counts(function(x) {x/sum(x)} ) %>% # Transform to rel. abundance
  psmelt()                                             # Melt to long format

ggplot(phylum.table, aes(x = Sample, y = Abundance, fill = Phylum))+
  facet_wrap(~ host_disease, scales = "free") + # scales = "free" removes empty lines
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(size = 5, angle = -90))+
  labs(x = "Samples", y = "Relative abundance")
```

For any further analysis, we will be doing it on the _physeq_shortASV_ object, as the ASVs sequences better reflect the V4 **variable** region.


### 7.3. Save to disk
```{r, eval=FALSE}
# Save to disk
saveRDS(raw_stats,      file.path(path.data, "01_Dada2-Zhu/raw_stats.rds"))
saveRDS(FWDpos.FWDread, file.path(path.data, "01_Dada2-Zhu/forwardreads_primerposition.rds"))
saveRDS(FWDpos.REVread, file.path(path.data, "01_Dada2-Zhu/reversereads_primerposition.rds"))
saveRDS(out2,           file.path(path.data, "01_Dada2-Zhu/out2.rds"))
saveRDS(errF,           file.path(path.data, "01_Dada2-Zhu/errF.rds"))
saveRDS(errR,           file.path(path.data, "01_Dada2-Zhu/errR.rds"))
saveRDS(dadaFs,         file.path(path.data, "01_Dada2-Zhu/infered_seq_F.rds"))
saveRDS(dadaRs,         file.path(path.data, "01_Dada2-Zhu/infered_seq_R.rds"))
saveRDS(mergers,        file.path(path.data, "01_Dada2-Zhu/mergers.rds"))


# Taxa & Phyloseq object
saveRDS(taxa,       file.path(path.data, "01_Dada2-Zhu/taxa_zhu_longASV.rds"))
saveRDS(taxa.small, file.path(path.data, "01_Dada2-Zhu/taxa_zhu_shortASV.rds"))

saveRDS(physeq,       file.path(path.data, "01_Dada2-Zhu/physeq_zhu_longASV.rds"))
saveRDS(physeq.small, file.path(path.root, "data/analysis-individual/CLUSTER/PhyloTree/input/physeq_zhu.rds")) # phyloseq object with shorter ASVs
saveRDS(physeq.small, file.path(path.root, "data/phyloseq-objects/phyloseq-without-phylotree/physeq_zhu.rds"))
```




******************
# 8. SESSION INFO
******************

```{r session-info}
sessionInfo()
```